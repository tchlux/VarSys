Interpolation for Modeling<br>and Predicting Variability
Preliminary defense document
:: Thomas C.H. Lux :: tchlux@vt.edu :: http://people.cs.vt.edu/tchlux

# Table of Contents

1. @@The Importance and Applications of Variability@@
1. @@Naive Approximations of Variability@@
1. @@Scalable Interpolants for Approximation@@
1. @@Strong Approximations of Variability@@
1. @@An Error Bound on Piecewise Linear Interpolation@@
1. @@Improving Variability Estimates with Monotone $C^2$ Splines@@

# Abstracts of Work

Each of high performance computing, cloud computing, and computer security have their own interests in modeling and predicting the performance of computers with respect to how they are configured. An effective model might infer internal mechanics, minimize power consumption, or maximize computational throughput of a given system. This paper analyzes a four-dimensional dataset measuring the input/output (I/O) characteristics of a cluster of identical computers using the benchmark IOzone. The I/O performance characteristics are modeled with respect to system configuration using multivariate interpolation and approximation techniques. The analysis reveals that accurate models of I/O characteristics for a computer system may be created from a small fraction of possible configurations, and that some modeling techniques will continue to perform well as the number of system parameters being modeled increases. These results have strong implications for future predictive analyses based on more comprehensive sets of system parameters.

A rapid increase in the quantity of data available is allowing all fields of science to generate more accurate models of multivariate phenomena. Regression and interpolation become challenging when the dimension of data is large, especially while maintaining tractable computational complexity. This paper proposes three novel techniques for multivariate interpolation and regression that each have polynomial complexity with respect to number of instances (points) and number of attributes (dimension). Initial results suggest that these techniques are capable of effectively modeling multivariate phenomena while maintaining flexibility in different application domains.


# The Importance and Applications of Variability

Performance tuning is often an experimentally complex and time intensive chore necessary for configuring HPC systems. The procedures for this tuning vary largely from system to system and are often subjectively guided by the system engineer(s). Once a desired level of performance is achieved, an HPC system may only be incrementally reconfigured as required by updates or specific jobs. In the case that a system has changing workloads or nonstationary performance objectives that range from maximizing computational throughput to minimizing power consumption and system variability, it becomes clear that a more effective and automated tool is needed for configuring systems. This scenario presents a challenging and important application of multivariate approximation and interpolation techniques.

Predicting the performance of an HPC system is a challenging problem that is primarily attempted in one of two ways: (1) build a statistical model of the performance by running experiments on the system at select settings, or (2) run artificial experiments using a simplified simulation of the target system to estimate architecture and application bottlenecks. In this section the proposed multivariate modeling techniques rest in the first category, and they represent a notable increase in the ability to model complex interactions between system parameters.

Many previous works attempting to model system performance have used simulated environments to estimate the performance of a system [[grobelny2007fase,wang2009simulation,wang2013towards]]. Some of these works refer to statistical models as being oversimplified and not capable of capturing the true complexity of the underlying system. This claim is partially correct, noting that a large portion of predictive statistical models rely on simplifying the model to one or two parameters [[snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing]]. These limited statistical models have provided satisfactory performance in very narrow application settings. Many of the aforementioned statistical modeling techniques claim to generalize, while simultaneously requiring additional code annotations, hardware abstractions, or additional application level understandings in order to generate models. The approach presented here requires no modifications of the application, no architectural abstractions, nor any structural descriptions of the input data being modeled. The techniques used are purely mathematical and only need performance data
as input.

Among the statistical models presented in prior works, [[bailey2005performance]] specifically mention that it is difficult for the simplified models to capture variability introduced by I/O. System variability in general has become a problem of increasing interest to the HPC and systems communities, however most of the work has focused on operating system (OS) induced variability [[beckman2008benchmarking,de2007identifying]]. The work that has focused on managing I/O variability does not use any sophisticated modeling techniques [[lofstead2010managing]]. Hence, this section presents a case study applying advanced mathematical and statistical modeling techniques to the domain of HPC I/O characteristics. The models are used to predict the mean throughput of a system and the variance in throughput of a system. The discussion section outlines how the techniques presented can be applied to any performance metric and any system.


# Naive Approximations of Variability

In general, this section compares five multivariate approximation techniques that operate on inputs in $\mathbb{R}^d$ ($d$-tuples of real numbers) and produce predicted responses in $\mathbb{R}$. In order to provide coverage of the varied mathematical strategies that can be employed to solve the continuous modeling problem, three of the techniques are regression based and the remaining two are interpolants. The sections below outline the mathematical formulation of each technique and provide computational complexity bounds with respect to the size (number of points and dimension) of input data. Throughout, $d$ will refer to the dimension of the input data, $n$ is the number of points in the input data, $x^{(i)} \in \mathbb{R}^d$ is the $i$-th input data point, $x^{(i)}_j$ is the $j$-th component of $x^{(i)}$, and $f(x^{(i)})$ is the response value of the $i$-th input data point.

In order to evaluate the viability of multivariate models for predicting system performance, this section presents a case study of a four-dimensional dataset produced by executing the IOzone benchmark from [[iozone]] on a homogeneous cluster of computers. All experiments were performed on parallel shared-memory nodes common to HPC systems. Each system had a lone guest Linux operating system (Ubuntu 14.04 LTS//XEN 4.0) on a dedicated 2TB HDD on a 2 socket, 4 core (2 hyperthreads per core) Intel Xeon E5-2623 v3 (Haswell) platform with 32 GB DDR4. The system performance data was collected by executing IOzone 40 times for each of a select set of system configurations. A single IOzone execution reports the max I/O throughput seen for the selected test in kilobytes per second. The 40 executions for each system configuration are converted into the mean and variance, both values in $\mathbb{R}$ capable of being modeled individually by the multivariate approximation techniques presented in Section Multivariate. The summary of data used in the experiments for this section can be seen in the @@System Configurations@@ Table. Distributions of raw throughput values being modeled can be seen in the @@Raw Throughput@@ Figure.

| | |
| **System Parameter** | **Values** |
| File Size | 64, 256, 1024 |
| Record Size | 32, 128, 512 |
| Thread Count | 1, 2, 4, 8, 16, 32, 64, 128, 256 |
| Frequency | \{12, 14, 15, 16, 18, 19, 20, 21, 23, 24, 25, 27, 28, 29, 30, 30.01\} $\times 10^5$ |
| | |
| **Response Values** | |
| Throughput Mean | [$2.6 \times 10^5$, $5.9 \times 10^8$] |
| Throughput Variance | [$5.9\times 10^{10} $, $4.7 \times 10^{16}$] |
:: A description of the system parameters being considered in the IOzone tests. Record size must not be greater than file size and hence there are only six valid combinations of the two. In total there are $6 \times 9 \times 16 = 864$ unique system configurations. :: System Configurations ::


{{HPC-Raw_Throughput.html}}
:: Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers. :: Raw Throughput ::

## Dimensional Analysis

This work utilizes an extension to standard $k$-fold cross validation
that allows for a more thorough investigation of the expected model
performance in a variety of real-world situations. Alongside
randomized splits, two extra components are considered: the amount of
training data provided, and the dimension of the input data. It is
important to consider that algorithms that perform well with less
training input also require less experimentation. Although, the amount
of training data required may change as a function of the dimension of
the input and this needs to be studied as well. The framework used
here will be referred to as a multidimensional analysis (MDA) of the
IOzone data.

### Multidimensional Analysis

This procedure combines random selection of training and testing
splits with changes in the input dimension and the ratio of training
size to testing size. Given an input data matrix with $n$ rows
(points) and $d$ columns (components), MDA proceeds as follows:

1. For all $k = 1$, $\ldots$, $d$ and for all nonempty subsets $F \subset \{1, 2, \ldots, d\}$, reduce the input data to points $(z, f_F(z))$ with $z \in \mathbb{R}^k$ and $f_F(z) = E\bigl[ \bigl\{ f\bigl(x^{(i)}\bigr) \bigm| \bigl(x^{(i)}_F = z\bigr) \bigr\} \bigr]$, where $E[\cdot]$ denotes the mean and $x^{(i)}_F$ is the subvector of $x^{(i)}$ indexed by $F$.
2. For all $r$ in $\{5, 10, \ldots, 95\}$, generate $N$ random splits $(train, test)$ of the reduced data with $r$ percentage for training and $100 - r$ percentage for testing.
3. When generating each of $N$ random $(train, test)$ splits, ensure that all points from $test$ are in the convex hull of points in $train$ (to prevent extrapolation); also ensure that the points in $train$ are well spaced.

In order to ensure that the testing points are in the convex hull of
the training points, the convex hull vertices of each set of (reduced
dimension) points are forcibly placed in the training set. In order to
ensure that training points are well spaced, a statistical method for
picking points from [[amos2014algorithm]] is used:

1. Generate a sequence of all pairs of points $\bigl(z^{(i_1)},z^{(j_1)}\bigr), \bigl(z^{(i_2)},z^{(j_2)}\bigr), \ldots$ sorted by ascending pairwise Euclidean distance between points, so that $\bigl|\bigl|z^{(i_k)}-z^{(j_k)}\bigr|\bigr|_2 \leq \bigl|\bigl|z^{(i_{k+1})}-z^{(j_{k+1})}\bigr|\bigr|_2$.
3. Sequentially remove points from candidacy until only $|train|$ remain by randomly selecting one point from the pair $\bigl(z^{(i_m)}, z^{(j_m)}\bigr)$ for $m = 1,\ldots$ if both $z^{(i_m)}$ and $z^{(j_m)}$ are still candidates for removal.

Given the large number of constraints, level of reduction, and use of
randomness in the MDA procedure, occasionally $N$ unique
training/testing splits may not be created or may not exist. In these
cases, if there are fewer than $N$ possible splits, then
deterministically generated splits are used. Otherwise after $3N$
attempts, only the unique splits are kept for analysis. The MDA
procedure has been implemented in Python\#3 while most regression and
interpolation methods are Fortran wrapped with Python. All randomness
has been seeded for repeatability.

For any index subset $F$ (of size $k$) and selected value of $r$, MDA
will generate up to $N$ multivariate models $f_F(z)$ and predictions
$\hat{f}_F\big(z^{(i)}\big)$ for a point $z^{(i)} \in \mathbb{R}^k$.
There may be fewer than $N$ predictions made for any given
point. Extreme points of the convex hull for the selected index subset
will always be used for training, never for testing. Points that do
not have any close neighbors will often be used for training in order
to ensure well-spacing. Finally, as mentioned before, some index
subsets do not readily generate $N$ unique training and testing
splits. The summary results presented in this work use the median of
the ($N$ or fewer) values $\hat{f}_F(z)$ at each point as the model
estimate for error analysis.

## Results

A naive multivariate prediction technique such as nearest
neighbor could experience relative errors in the range $\displaystyle
[0, \big(\max_x f(x) - \min_x f(x)\big) / \min_x f(x) ]$ when modeling
a nonnegative function $f(x)$ from data. The IOzone mean data response
values span three orders of magnitude (as can be seen in the
@@System Configurations@@ Table) while variance data response values span six
orders of magnitude. It is expected therefore, that all studied
multivariate models perform better than a naive approach,
achieving relative errors strictly less than $10^3$ for throughput
mean and $10^6$ for throughput variance. Ideally, models will yield
relative errors significantly smaller than 1. The time required to
compute thousands of models involved in processing the IOzone data
through MDA was approximately five hours on a CentOS workstation with
an Intel i7-3770 CPU at 3.40GHz. In four dimensions for example, each
of the models could be constructed and evaluated over hundreds of
points in less than a few seconds.

### I/O Throughput Mean

Almost all multivariate models analyzed make predictions with a
relative error less than 1 for most system configurations when
predicting the mean I/O throughput of a system given varying amounts
of training data. The overall best of the multivariate models,
Delaunay, consistently makes predictions with relative error less than
$.05$ (5\% error). In Figure Mean tt Ratio it can also be
seen that the Delaunay model consistently makes good predictions even
with as low as 5\% training data (43 of the 864 system configurations)
regardless of the dimension of the data.

### I/O Throughput Mean

Almost all multivariate models analyzed make predictions with a
relative error less than 1 for most system configurations when
predicting the mean I/O throughput of a system given varying amounts
of training data. The overall best of the multivariate models,
Delaunay, consistently makes predictions with relative error less than
$.05$ (5\% error). In the @@Mean Train/Test Ratio@@ Figure it can also be
seen that the Delaunay model consistently makes good predictions even
with as low as 5\% training data (43 of the 864 system configurations)
regardless of the dimension of the data.

{{HPC-Mean_Dim.html}}
:: These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated. :: Mean Dimension ::

$ $

{{HPC-Mean_TT_Ratio.html}}
:: These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size. :: Mean Train/Test Ratio ::

$ $

{{HPC-Var_TT_Ratio.html}}
:: These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size. :: Variance Train/Test Ratio ::


### I/O Throughput Variance

predicting mean. Delaunay remains the best overall predictor
(aggregated across training percentages and dimensions) with median
relative error of .47 and LSHEP closely competes with Delaunay having
a median signed relative error of -.92. Outliers in prediction error
are much larger for all models. Delaunay produces relative errors as
large as 78 and other models achieve relative errors around
$10^3$. The relative errors for many models scaled proportional to the
increased orders of magnitude spanned by the variance response
compared with mean response. As can be seen in Figure @@Variance Train/Test Ratio@@, all models are more sensitive to the amount of
training data provided than their counterparts for predicting mean.


### Increasing Dimension and Decreasing Training Data

As can be seen in the @@Mean Dim@@ Figure, all of the models suffer
increasing error rates in higher dimension. This is expected, because
the number of possible interactions to model grows
exponentially. However, LSHEP and Delaunay maintain the slowest
increase in relative error. The increase in error seen for Delaunay
suggests that it is capable of making predictions with a range of
relative errors that grows approximately linearly with increasing
dimension input. This trend suggests that Delaunay would remain a
viable technique for accurately modeling systems with 10's of
parameters given only small amounts of training data. All models, with
the exception of MARS, produce smaller errors given more training
data. Increasing the amount of training data most notably reduces the
number of prediction error outliers.


## Discussion of Results

The present results demonstrate that a straightforward application of
multivariate modeling techniques can be used to effectively predict
HPC system performance. Some modeling effort on the part of a systems
engineer combined with a significant amount of experimentation (days
of CPU time for the IOzone data used here) can yield a model capable
of accurately tuning an HPC system to the desired performance
specification, although qualitatively correct predictions can be
achieved with much less (10\%, say) effort.


### Modeling the System

The modeling techniques generated estimates of drastically different
quality when predicting I/O throughput mean and variance. A few
observations: SVR has the largest range of errors for all selections
of dimension and amounts of training data; MARS and LSHEP produce
similar magnitude errors while the former consistently underestimates
and the latter consistently overestimates; Delaunay has considerably
fewer outliers than all other methods. SVR likely produces the poorest
quality predictions because the underlying parametric representation
is global and oversimplified (a single polynomial), making it unable
to capture the complex local behaviors of system I/O. It is still
unclear, however, what causes the behaviors of LSHEP, MARS, and
Delaunay. An exploration of this topic is left to future work.

While the Delaunay method appears to be the best predictor in the
present IOzone case study, it is important to note that the Delaunay
computational complexity increases with the dimension of the input
more rapidly than other techniques. The implementation of Delaunay
(QuickHull) used would experience unacceptably large training times
beyond ten-dimensional input. This leaves much room for other
techniques to perform best in higher dimension unless a more efficient
implementation of Delaunay can be used.

Finally, the ability of the models to predict variance was
significantly worse than for the I/O mean. The larger scale in
variance responses alone do not account for the increase in relative
errors witnessed. This suggests that system variability has a greater
underlying functional complexity than the system mean and that latent
factors are reducing prediction performance.

### Extending the Analysis

System I/O throughput mean and variance are simple and useful system
characteristics to model. The process presented in this work is
equally applicable to predicting other useful performance
characteristics of HPC systems such as: computational throughput,
power consumption, processor idle time, context switches, RAM usage,
or any other ordinal performance metric. For each of these there is
the potential to model system variability as well. This work has
chosen variance as a measure of variability, but the techniques used
in this paper could be applied to more precise measures of variability
such as the percentiles of the performance distribution or the entire
distribution itself. A thorough exploration of HPC systems
applications of multivariate modeling constitutes future work.


## Conclusion of This Section

Multivariate models of HPC system performance can effectively predict
I/O throughput mean and variance. These multivariate techniques
significantly expand the scope and portability of statistical models
for predicting computer system performance over previous work. In the
IOzone case study presented, the Delaunay method produces the best
overall results making predictions for 821 system configurations with
less than 5\% error when trained on only 43 configurations. Analysis
also suggests that the error in the Delaunay method will remain
acceptable as the number of system parameters being modeled
increases. These multivariate techniques can and should be applied to
HPC systems with more than four tunable parameters in order to
identify optimal system configurations that may not be discoverable
via previous methods nor by manual performance tuning.

The most severe limitation to the present work is the restriction to
modeling strictly ordinal (not categorical) system parameters.
Existing statistical approaches for including categorical variables
are inadequate for nonlinear interactions in high dimensions. Future
work could attempt to identify the viability of different techniques
for making predictions including categorical system parameters.

There remain many other multivariate modeling techniques not included
in this work that should be evaluated and applied to HPC performance
prediction. For I/O alone, there are far more than the four tunable
parameters studied in this work. Alongside experimentation with more
models, there is room for a theoretical characterization of the
combined model and data properties that allow for the greatest
predictive power.


# Scalable Interpolants for Approximation

# Strong Approximations of Variability

# An Error Bound on Piecewise Linear Interpolation

# Improving Variability Estimates with Monotone $C^2$ Splines

This portion of the document proposes research on the numerically robust construction of monotone quintic interpolating splines from data that are $C^2$.

## Introduction and Motivation

Many domains of science rely on smooth approximations to real-valued functions over a closed interval. These smooth approximations are regularly used in automotive and aerospace engineering, architecture, mathematics and especially statistics [[knott2012interpolating]]. While polynomial interpolants or regressors apply broadly, piecewise polynomial functions (splines) are often a good choice because they can approximate globally complex functions while minimizing local complexity of the approximation. It is often the case that the true underlying function or phenomenon being modeled has known properties e.g., convexity, positivity, various levels of continuity, or monotonicity. Given a reasonably large amount of data, it can be impossible to maintain these properties with a single polynomial function.

In general, the maintenance of function properties through interpolation / regression is usually referred to as *shape preserving* [[fritsch1980monotone,gregory1985shape]]. The specific shapes this work will focus on are monotonicity and multiple levels of continuity for a function. These properties are chiefly important to the approximation of cumulative distribution functions and subsequently the effective generation of random numbers from a specified distribution. 

In statistics especially, the construction of a monotone interpolating spline that is continuous in second derivative is meaningfully useful [[ramsay1988monotone]]. A function with these properties could approximate random variables to a high level of accuracy with relatively few intervals((with even more accuracy given greater continuity)). A continuously twice differentiable approximation to a cumulative distribution function would also produce a corresponding probability density function that is continuously differentiable, which is a property many commonly occurring parametric distributions maintain.


## Related Work

The current state-of-the-art monotone interpolating spline with a mathematical software implementation is piecewise cubic, continuously differentiable, and was first proposed in [[fritsch1980monotone]] then expanded upon in [[carlson1985monotone]]. Let $\pi: a = x_1 < x_2 < \cdots < x_n = b$ be a partition of the interval $[a,b]$. Let $\{f(x_i) : i = 1,2,\ldots,n\}$ be a given set of data values at the partition points for a monotone function $f$, meaning $f(x_i) \leq f(x_{i+1})$ for $i = 1, \ldots, n-1$ or $f(x_i) \geq f(x_{i+1})$ for $i = 1, \ldots, n-1$. Let $\hat f$ be a piecewise cubic spline defined in each sub-interval $I_i = [x_i, x_{i+1}]$ by

$$ \begin{align}

h_i =& \ x_{i+1} - x_{i} \\
u(t) =& \ 3t^2 - 2t^3 \\
p(t) =& \ t^3 - t^2 \\
\hat f(x) =& \ f(x_i)\ u\big((x_{i+1} - x) / h_i\big) + f(x_{i+1})\ u\big((x - x_i) / h_i\big) \\
& - \hat f^{\ \prime}(x_i)\ p\big((x_{i+1}-x)/h_i\big) + \hat f^{\ \prime}(x_{i+1})\ p\big((x-x_i)/h_i \big).

\end{align} $$

Notice that it is up to the user to choose values for $\hat f^{\ \prime}$and a viable monotonic cubic spline can be produced by choosing $\hat f^{\ \prime}(x_i) = 0$, $i = 1, \ldots, n$. However, such a spline has too many *wiggles* for most applications. Fritsch and Carlson proceed to show that simple conditions on the derivative values can guarantee monotonicity, and that these conditions can be enforced in a way that ensures modifications on one interval will not break the monotonicity of cubic polynomials over any neighboring intervals. Consider the terms $\alpha = \frac{\hat f^{\ \prime}(x_i) (x_{i+1}-x_i)}{f(x_{i+1}) - f(x_i)}$ and $\beta = \frac{\hat f^{\ \prime}(x_{i+1}) (x_{i+1}-x_i)}{f(x_{i+1}) - f(x_i)}$, now monotonicity of a cubic polynomial over a sub-interval can be maintained by ensuring that $\alpha$ and $\beta$ reside in any of the following regions.

{{http://people.cs.vt.edu/tchlux/files/Dissertation/feasible_region.html}}

The actual region of monotonicity for a cubic polynomial is larger, but projection of $(\alpha, \beta)$ into one of these regions ensures that monotonicity will be achieved and not violated for neighboring regions. The user must decide which region is most suitable to project $(\alpha, \beta)$ onto, Fritsch and Carlson recommend using region 2.

This theoretical work has been extended in [[ulrich1994positivity,hess1994positive]] to monotone quintic polynomials, but no mathematical software has been produced accordingly. That is where this proposed work comes in.


## Achieved Progress

To expand my own understanding of the current state-of-the-art method for constructing a monotone spline interpolant, I {#37a}@{fully implemented}{https://github.com/tchlux/VarSys/blob/master/Disseration/cubic.py}@{#37a} the algorithm described in [[fritsch1980monotone]] in Python. This code can be used to generate monotone cubic spline fits to data and allows for the user specification of a method((the means by which monotonicity is enforced)) and derivatives at any of the knots or endpoints. Clearly this code is not optimized for numerical robustness, nor speed, but it serves as a portable demonstration and educational tool.

Here we can see the recommended method of projection for moving cubic polynomials into the region of monotonicity.

{{http://people.cs.vt.edu/tchlux/files/Dissertation/demo_projection.html}}

The resulting interpolating spline is $C^1$ and has a *smooth* appearance, here is a demonstration when using Region 2.

{{http://people.cs.vt.edu/tchlux/files/Dissertation/demo_fit.html}}

And the derivative of the above spline is continuous.

{{http://people.cs.vt.edu/tchlux/files/Dissertation/demo_fit_deriv.html}}

## Research Goal

{#3a7} ****Produce numerically robust software that computes monotone quintic interpolating splines given data.**** {#3a7}

## Timeline

|  |							|
| **Date**	 | **Milestone**						|
| *June<br>2019*	 | Implementation of monotone quintic polynomial *interpolant*. |
| *August<br>2019*	 | Implementation of monotone quintic polynomial *spline*.	|
| *October<br>2019* | First draft of TOMS paper on algorithm.			|
| *December<br>2019*| Research Defense of work.					|
| *March<br>2020*	 | Submission of TOMS paper and code.				|
| *April<br>2020*	 | Final Defense of Ph.D.					|

## Final Remarks

This project has the potential to be very widely utilized. As described earlier, many statistical applications could greatly benefit from having a $C^2$ approximation to CDFs, because the resulting PDF will be both aesthetically pleasing and more accurate for continuous distributions.

I spent last summer preparing a @{mathematical software}{https://github.com/tchlux/VarSys/blob/master/Code_[2018-06-11]_Stable_Arbitrary_Box_Splines/boxspline.f90}@ for the evaluation of box-splines, which may (or may not) be included in this package as well.


----------------------------------------------------------------------
# Important to Know
## General Matrix Operations
- QR 
- LU
- SVD 
- Diagonalization
## Under Special Conditions
- Spectral Decomposition for Symmetric
- Cholesky
- Shor Decomposition
----------------------------------------------------------------------


======================================================================

%% -------------------------------------------------------------------
%%                           HPC Paper

%% This article focuses on predive modelling using a simulation
%% environment, citing analytic modelling as "over-simplified"
%% in order to make the problem solveable. Their framework
%% relies almost entirely on simulation in order to estimate
%% system performance.
@article{grobelny2007fase,
  title={FASE: A framework for scalable performance prediction of HPC systems and applications},
  author={Grobelny, Eric and Bueno, David and Troxel, Ian and George, Alan D. and Vetter, Jeffrey S.},
  journal={Simulation},
  volume={83},
  number={10},
  pages={721--745},
  year={2007},
  publisher={Sage Publications Sage UK: London, England}
}

%% A citation from the original VarSys proposal that uses
%% simulation to estimate system performance.
@inproceedings{wang2009simulation,
  title={A simulation approach to evaluating design decisions in mapreduce setups},
  author={Wang, Guanying and Butt, Ali R. and Pandey, Prashant and Gupta, Karan},
  booktitle={IEEE International Symposium on Modeling, Analysis \&
                  Simulation of Computer and Telecommunication Systems
                  (MASCOTS'09)},
  pages={1--11},
  year={2009},
  organization={IEEE}
}

%% A citation from the varsys proposal that uses simulation to
%% predict system performance.
@inproceedings{wang2013towards,
  title={Towards improving mapreduce task scheduling using online simulation based predictions},
  author={Wang, Guanying and Khasymski, Aleksandr and Krish, K. R. and Butt, Ali R.},
  booktitle={International Conference on Parallel and Distributed
                  Systems (ICPADS), 2013},
  pages={299--306},
  year={2013},
  organization={IEEE}
}

%% This is an old article, the authors argue it is best to use
%% the fewest possible parameters to model the expected
%% performance of a systme. They subjectively decided those
%% parameters (two for system, a few more for applications) to
%% predict performance based on memory / network bottlenecks.
@inproceedings{snavely2002framework,
  title={A framework for performance modeling and prediction},
  author={Snavely, Allan and Carrington, Laura and Wolter, Nicole and Labarta, Jesus and Badia, Rosa and Purkayastha, Avi},
  booktitle={Supercomputing, ACM/IEEE Conference},
  pages={21--21},
  year={2002},
  organization={IEEE}
}

%% This paper presents a continuation of the PERC work that
%% predicts the time-performance of a machine and an application
%% given a detailed hardware specification along with an
%% application description. This paper EXCPLICITLY MENTIONS
%% "..models do not do a good job of modeling I/O.." They don't
%% say how the model is constructed, only that they use
%% "statistical methods".
@inproceedings{bailey2005performance,
  title={Performance modeling: Understanding the past and predicting the future},
  author={Bailey, David H and Snavely, Allan},
  booktitle={European Conference on Parallel Processing},
  pages={185--195},
  year={2005},
  organization={Springer}
}

%% This article is an extension of snavely2002framework, using
%% only high level system architecture characterstics combined
%% with a detailed application study to predict how long a given
%% application will take to run on given hardware. It focuses a
%% lot more on the subjectively chosen best ways to combine
%% knowledge about a system and application into a naively
%% generated performance prediction (relying on linear relationships)
@article{barker2009using,
  title={Using performance modeling to design large-scale systems},
  author={Barker, Kevin J. and Davis, Kei and Hoisie, Adolfy and Kerbyson, Darren J. and Lang, Michael and Pakin, Scott and Sancho, Jos{\'e} Carlos},
  journal={Computer},
  volume={42},
  number={11},
  year={2009},
  publisher={IEEE}
}

%% This article is focused on analyzing the viability of virtual
%% machine useage in HPC settings. They say that is possible,
%% and they have created "non-linear regression models" of the
%% system performance. These models are only over 1 variable at
%% a time, they are not function of multiple variables.
@inproceedings{ye2010analyzing,
  title={Analyzing and modeling the performance in xen-based virtual cluster environment},
  author={Ye, Kejiang and Jiang, Xiaohong and Chen, Siding and Huang, Dawei and Wang, Bei},
  booktitle={12th IEEE International Conference on High Performance Computing and Communications (HPCC)},
  pages={273--280},
  year={2010},
  organization={IEEE}
}


%% Variability in OS jitter citation from VarSys
@article{beckman2008benchmarking,
  title={Benchmarking the effects of operating system interference on extreme-scale parallel machines},
  author={Beckman, Pete and Iskra, Kamil and Yoshii, Kazutomo and Coghlan, Susan and Nataraj, Aroon},
  journal={Cluster Computing},
  volume={11},
  number={1},
  pages={3--16},
  year={2008},
  publisher={Springer}
}

%% Variability in OS Jitter citation from VarSys
@inproceedings{de2007identifying,
  title={Identifying sources of operating system jitter through fine-grained kernel instrumentation},
  author={De, Pradipta and Kothari, Ravi and Mann, Vijay},
  booktitle={IEEE International Conference on Cluster Computing},
  pages={331--340},
  year={2007},
  organization={IEEE}
}

%% Variability in I/O from VarSys
@inproceedings{lofstead2010managing,
  title={Managing variability in the IO performance of petascale storage systems},
  author={Lofstead, Jay and Zheng, Fang and Liu, Qing and Klasky, Scott and Oldfield, Ron and Kordenbrock, Todd and Schwan, Karsten and Wolf, Matthew},
  booktitle={International Conference on High Performance Computing,
                  Networking, Storage and Analysis (SC), 2010},
  pages={1--12},
  year={2010},
  organization={IEEE}
}

@online{iozone,
  author={Norcott, W. D.},
  title={IOzone Filesystem Benchmark},
  year=2017,
  url={http://www.iozone.org},
  note = {http://www.iozone.org [Online; accessed 2017-11-12]},
  urldate={2017-11-12},
}

%% QNSTOP algorithm
@article{amos2014algorithm,
  title={Algorithm XXX: QNSTOPâ€”quasi-Newton algorithm for stochastic optimization},
  author={Amos, Brandon D. and Easterling, David R. and Watson, Layne T. and Thacker, William I. and Castle, Brent S. and Trosset, Michael W.},
  journal={Technical Report 14-02, Dept. of Computer Science, VPI\&SU, Blacksburg, VA},
  year={2014},
}

%% -------------------------------------------------------------------
%%                         Proposed New Work

@article{fritsch1980monotone,
  author = {Fritsch, F. and Carlson, R.},
  title = {Monotone Piecewise Cubic Interpolation},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {17},
  number = {2},
  pages = {238-246},
  year = {1980},
  doi = { 10.1137/0717021 },
  URL = { https://doi.org/10.1137/0717021 },
  eprint = { https://doi.org/10.1137/0717021 },
}


@article{carlson1985monotone,
  author = {Carlson, R. and Fritsch, F.},
  title = {Monotone Piecewise Bicubic Interpolation},
  journal = {SIAM Journal on Numerical Analysis},
  volume = {22},
  number = {2},
  pages = {386-400},
  year = {1985},
  doi = {10.1137/0722023},
  URL = { https://doi.org/10.1137/0722023 },
  eprint = { https://doi.org/10.1137/0722023 },
}

@article{ramsay1988monotone,
  title={Monotone Regression Splines in Action},
  author={Ramsay, James O and others},
  journal={Statistical science},
  volume={3},
  number={4},
  pages={425--441},
  year={1988},
  publisher={Institute of Mathematical Statistics}
}

@article{gregory1985shape,
  title={Shape preserving spline interpolation},
  author={Gregory, John A},
  journal = {Brunel University},
  year={1985},
  url = { https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19850020252.pdf }
}

%% Dr. Watson's paper on monotone quintic spline interpolation.
@article{ulrich1994positivity,
  author = {Ulrich, G. and Watson, L.},
  title = {Positivity Conditions for Quartic Polynomials},
  journal = {SIAM Journal on Scientific Computing},
  volume = {15},
  number = {3},
  pages = {528-544},
  year = {1994},
  doi = { 10.1137/0915035 },
  URL = { https://doi.org/10.1137/0915035 },
  eprint = { https://doi.org/10.1137/0915035 },
}

%% A second paper published the same year as Dr. Watson's on quintic interpolation.
@article{hess1994positive,
  title={Positive quartic, monotone quintic C2-spline interpolation in one and two dimensions},
  author={Hess, Walter and Schmidt, Jochen W},
  journal={Journal of Computational and Applied Mathematics},
  volume={55},
  number={1},
  pages={51--67},
  year={1994},
  doi={ 10.1016/0377-0427(94)90184-8 },
  publisher={Elsevier}
}

%% Another paper on monotone quintics. This paper implements an algorithm for constructing a quintic interpolant.
@article{dougherty1989nonnegativity,
  title={Nonnegativity-, monotonicity-, or convexity-preserving cubic and quintic Hermite interpolation},
  author={Dougherty, Randall L and Edelman, Alan S and Hyman, James M},
  journal={Mathematics of Computation},
  volume={52},
  number={186},
  pages={471--494},
  year={1989}
}

@book{knott2012interpolating,
  title={Interpolating cubic splines},
  author={Knott, Gary D},
  volume={18},
  year={2012},
  doi = { 10.1007/978-1-4612-1320-8 },
  publisher={Springer Science \& Business Media}
}


